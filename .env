HF_TOKEN=
HF_XET_HIGH_PERFORMANCE=1
SECRET_TOKEN=changeme

# Model config
# Set this to your local fused checkpoint directory
MODEL_NAME=fan02942/fused-glm-4.6
TORCH_DTYPE=bf16
TRUST_REMOTE_CODE=true
DEVICE_MAP=auto

# Attention implementation (sdpa, flash_attention_2, eager)
ATTN_IMPLEMENTATION=sdpa

# Tokenization/padding
TOKENIZER_PADDING_SIDE=left
PAD_TO_MULTIPLE_OF=16
MAX_INPUT_TOKENS=512
ALLOWED_MAX_NEW_TOKENS=64,128,256,512

# Optional advanced optimization
STATIC_KV_CACHE=false

# Grouped GEMM / MoE optimization
USE_GROUPED_GEMM=true
# Using a pre-fused checkpoint
LOAD_FUSED_EXPERTS=true
# Force CPU-first load then dispatch across GPUs
FUSE_ON_CPU_BEFORE_SHARD=true

# Constrained vocab prebuild
PREBUILD_PREFIX=true
PREBUILD_WORD_COUNTS=1000
PREBUILD_LANGS=es

# Batch job
BATCH_JOB_PIPELINE_SIZE=4

# In-process GPU generation concurrency (1 = fully serialized)
GENERATION_MAX_CONCURRENCY=1

# Where to find wordlists (defaults to ./wordlists)
WORDLIST_DIR=wordlists

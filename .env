HF_TOKEN=
HF_XET_HIGH_PERFORMANCE=1
SECRET_TOKEN=changeme

# Model config
MODEL_NAME=/fusedmodel/fused-GLM-4.6
TORCH_DTYPE=bf16
TRUST_REMOTE_CODE=true

# Force initial CPU load then shard to multiple GPUs
CPU_FIRST_LOAD=true
DEVICE_MAP=auto

# Memory mapping for Accelerate (H200 SXM ~141GiB each; reserve ~6GiB headroom)
# Adjust cuda:N GiB values lower if still seeing OOM.
MAX_MEMORY=cuda:0=135GiB,cuda:1=135GiB,cuda:2=135GiB,cuda:3=135GiB,cuda:4=135GiB,cuda:5=135GiB,cpu=64GiB

# Optional: prevent splitting specific module classes (names from remote model code)
# If unsure, leave blank or add classes once you inspect the model.
# NO_SPLIT_MODULE_CLASSES=GLM4MoEDecoderLayer

# Attention implementation (sdpa, flash_attention_2, eager)
ATTN_IMPLEMENTATION=sdpa

# Tokenization/padding
TOKENIZER_PADDING_SIDE=left
PAD_TO_MULTIPLE_OF=16
MAX_INPUT_TOKENS=512
ALLOWED_MAX_NEW_TOKENS=64,128,256,512

# Optional advanced optimization
STATIC_KV_CACHE=false

# Grouped GEMM / MoE optimization
USE_GROUPED_GEMM=true
LOAD_FUSED_EXPERTS=true
FUSE_ON_CPU_BEFORE_SHARD=false

# Constrained vocab prebuild
PREBUILD_PREFIX=true
PREBUILD_WORD_COUNTS=1000
PREBUILD_LANGS=es

# Batch job
BATCH_JOB_PIPELINE_SIZE=4

# Generation concurrency
GENERATION_MAX_CONCURRENCY=1

# Wordlists directory
WORDLIST_DIR=wordlists

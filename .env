# Auth
HF_TOKEN=
HF_XET_HIGH_PERFORMANCE=1
SECRET_TOKEN=changeme

# Model config
MODEL_NAME=TheDrummer/Cydonia-24B-v4.2.0
TORCH_DTYPE=bf16
TRUST_REMOTE_CODE=true

# Use "auto" to let HF shard across multiple GPUs after CPU fusion.
# Or set a specific device like "cuda:0" for single GPU.
DEVICE_MAP=auto

# Attention implementation (sdpa, flash_attention_2, eager)
ATTN_IMPLEMENTATION=sdpa

# Tokenization/padding
TOKENIZER_PADDING_SIDE=left
PAD_TO_MULTIPLE_OF=16
MAX_INPUT_TOKENS=512
ALLOWED_MAX_NEW_TOKENS=64,128,256,512

# Optional advanced optimization
STATIC_KV_CACHE=false

# Grouped GEMM / MoE
USE_GROUPED_GEMM=true
# If the checkpoint you load is already fused and saved (e.g., via an offline script), set true.
LOAD_FUSED_EXPERTS=false
# Force: load model on CPU, fuse on CPU, then shard across GPUs.
FUSE_ON_CPU_BEFORE_SHARD=true

# Multi-GPU sharding hints (optional but recommended to avoid one-GPU OOM)
# Format: "gpu0:70GiB,gpu1:70GiB,cpu:256GiB"
# Leave unset to auto-detect ~90% of free VRAM per GPU.
# GPU_MAX_MEMORY=gpu0:70GiB,gpu1:70GiB
# CPU_MAX_MEMORY=256GiB
# Where to offload if needed during dispatch
OFFLOAD_DIR=.offload

# Constrained vocab prebuild
PREBUILD_PREFIX=true
PREBUILD_WORD_COUNTS=1000
PREBUILD_LANGS=es

# Batch job
BATCH_JOB_PIPELINE_SIZE=4

# In-process GPU generation concurrency (1 = fully serialized)
GENERATION_MAX_CONCURRENCY=1

# Where to find wordlists (defaults to ./wordlists)
WORDLIST_DIR=wordlists

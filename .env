HF_TOKEN=
HF_XET_HIGH_PERFORMANCE=1
SECRET_TOKEN=changeme

# Model config
MODEL_NAME=/fusedmodel/fused-GLM-4.6
TORCH_DTYPE=bf16
TRUST_REMOTE_CODE=true

# Force initial CPU load even for prefused checkpoints, then shard to GPUs
CPU_FIRST_LOAD=true
DEVICE_MAP=auto

# Attention implementation (sdpa, flash_attention_2, eager)
ATTN_IMPLEMENTATION=sdpa

# Tokenization/padding
TOKENIZER_PADDING_SIDE=left
PAD_TO_MULTIPLE_OF=16
MAX_INPUT_TOKENS=512
ALLOWED_MAX_NEW_TOKENS=64,128,256,512

# Optional advanced optimization
STATIC_KV_CACHE=false

# Grouped GEMM / MoE optimization
# Using a prefused checkpoint (LOAD_FUSED_EXPERTS=true + use_grouped_gemm true).
USE_GROUPED_GEMM=true
LOAD_FUSED_EXPERTS=true
FUSE_ON_CPU_BEFORE_SHARD=false  # retained for backward compatibility, not required now

# Constrained vocab prebuild
PREBUILD_PREFIX=true
PREBUILD_WORD_COUNTS=1000
PREBUILD_LANGS=es

# Batch job
BATCH_JOB_PIPELINE_SIZE=4

# In-process GPU generation concurrency (1 = fully serialized)
GENERATION_MAX_CONCURRENCY=1

# Where to find wordlists (defaults to ./wordlists)
WORDLIST_DIR=wordlists

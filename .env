# Auth
HF_TOKEN=
# API auth
# IMPORTANT: Set a strong token in production.
SECRET_TOKEN=changeme

# Model config
MODEL_NAME=bash99/Qwen3-30B-A3B-Instruct-2507-fused-bnb-4bit
TORCH_DTYPE=bf16          # bf16 on Ampere+; use float16 on older GPUs; "auto" is also fine
TRUST_REMOTE_CODE=true   # enable only for trusted model repos
DEVICE_MAP=cuda

# Attention implementation (sdpa, flash_attention_2, eager)
ATTN_IMPLEMENTATION=sdpa

# Tokenization/padding
TOKENIZER_PADDING_SIDE=left
# Prefer 8 or 16 unless you run kernels that benefit from 64
PAD_TO_MULTIPLE_OF=16
MAX_INPUT_TOKENS=512
ALLOWED_MAX_NEW_TOKENS=64,128,256,512

# Optional advanced optimization
STATIC_KV_CACHE=false

# Constrained vocab prebuild
PREBUILD_PREFIX=true
PREBUILD_WORD_COUNTS=3000
PREBUILD_LANGS=es

# Batch job
BATCH_JOB_PIPELINE_SIZE=4

# Where to find wordlists (defaults to ./wordlists)
WORDLIST_DIR=wordlists

# Qwen3 MoE fused backend
MOE_FUSED_ENABLE=true
# Set true to see Triton autotuning logs (can help performance/stability debugging)
MOE_FUSED_TRITON_AUTOTUNING=true

# Optional: directory for batch job temp files (defaults to OS temp)
# BATCH_JOB_TEMP_DIR=/tmp

# HF cache dir (optional)
# HF_HOME=/workspace/huggingface-cache
